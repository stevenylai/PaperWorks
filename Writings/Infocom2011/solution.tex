\section{Proposed Methods for Energy Efficient Clustering}
\label{sec:solution}
\subsection{Two Centralized Algorithms}
\label{sec:centralizedsolution}
Two centralized algorithms are proposed to solve our clustering problem. These two algorithms use the similar idea of the greedy algorithm for the set cover problem but adopt different approaches to handle the extra constraints of clustering. In both of the algorithms, a set of candidate single-hop clusters is first established given the network  \(G =(V,E)\). Then the most cost-effective cluster is selected from this set, one at a time, until all the sensor nodes in \(V\) have been covered.  

In the first algorithm, to find a candidate cluster set \(U\), we first calculate the optimal cluster size \(n_{opt}\) according to Eq. \ref{eq:MagicNumberOverlapping2}. Then based on \(n_{opt}\), one-hop neighbors of each node in \(V\) are partitioned. For each node \(s_i \in V\), assume the one-hop neighbor set is \(Ne_{s_i}\), if \( \left|Ne_{s_i}\right| \geq n_{opt}-1\), then each cluster in the cluster set contains a common element \(s_i\) and the remaining elements are the combinations of nodes in \(Ne_{s_i}\) with the length of \(n_{opt}-1\). When \(\left|Ne_{s_i}\right| < n_{opt} -1\), \(C_i =\{s_i\} \bigcup Ne_{s_i}\). Note that we assume the network is dense enough such that each sensor node has at least \(p\) one-hop neighbors. The obtained cluster sets for all the nodes in \(V\) are combined together to obtain the candidate cluster set \(U\).

The algorithm then selects the most cost-effective cluster \(S_i \in U\), one at a time, until all the sensor nodes in \(V\) have been covered. The cost effectiveness, denoted as \(\lambda\), is defined as \(\lambda = \frac{1}{\left|S_i\cup C_a\right| - \left|C_a\right|}\), where \(C_a\) represents the set of nodes covered so far. When selecting the most cost-effective cluster, we choose from the clusters in \(U\) which overlap with \(C_a\). This strategy can ensure that all the selected sensor nodes will be connected through the overlapping nodes.  If more than one candidate clusters which overlap with \(C_a\) have the same \(\lambda\), the one which maximizes the total degrees of the remaining un-covered nodes (i.e. \(U- S_i\bigcup C_a\)) will be chosen. It can be seen that this algorithm divides the sensor nodes in \(V\) into as many single-hop clusters of size \(n_{opt}\) as possible while keeps the number of overlapping nodes into minimums (from \(\lambda = \frac{1}{\left|S_i\cup C_a\right| - \left|C_a\right|}\), penalty is given to cluster having large number of overlapping nodes with \(C_a\)). Both of these two points are of importance to minimize the overall energy cost. The algorithm is shown as Algorithm \ref{algo:greedy1}.

The second algorithm uses different strategy to handle overlapping.  First, the optimal cluster size \(n_{opt}\) is calculated based on Eq. \ref{eq:MagicNumberOverlapping} without considering overlapping constraint. When selecting the most cost effective cluster, it is chosen from all the candidate clusters in \(U\). Since the overlapping constraint is not considered when selecting cluster, after all the sensor nodes in \(V\) have been covered, the algorithm will test if all the clusters are connected through the overlapping nodes and add extra clusters to connect them if necessary. The basic idea is to identify all the isolated cluster groups and then find clusters to connect them. The detailed description is omitted for brevity. This algorithm is shown as Algorithm \ref{algo:greedy2}. 

\begin{comment}
To achieve this, all the isolated cluster groups (ICGs) from the obtained clusters are identified . Clusters within an ICG are connected through overlapping nodes but do not overlap with other ICGs. Each ICG is then represented as a vertex in a graph \(G_{ICG}\). There is an edge between two vertices in \(G_{ICG}\) if there exist two nodes, each within a ICG corresponding to one of the two vertices, are within the communication range of each other. Then the minimum spanning tree (MST) is found that can connect all the vertices in \(G_{ICG}\). For each edge in the MST, an extra cluster is established in which the two sensor nodes associated with the edge are included. If \(p \geq 2\), then additional nodes should also be added to satisfy the constraint 3 of the optimal clustering problem. This algorithm is shown as Algorithm \ref{algo:greedy2}.
\end{comment}

\begin{algorithm}
\begin{algorithmic}[1]
\REQUIRE \(G=(V, E)\) and parameters listed in Table. \ref{tab:Table2}
\STATE find \(n_{opt}\) which minimizes Eq. \ref{eq:MagicNumberOverlapping}
	\STATE \(U\gets \emptyset\) \(C_a\gets \emptyset\)
	\FORALL {\(n_i\in V\)}
		\STATE \(S_i\gets \emptyset\)
		\FORALL {one hop neighbor \(n_j\) of \(n_i\)}
			\STATE \(S_i = S_i \bigcup \{n_j\}\)
		\ENDFOR
		\STATE construct a cluster set \(C_i\) whose elements are the combinations taken of the nodes in \(S_i\) of length \(n_{opt} - 1\).
		\STATE \(U=U\bigcup C_i\)
	\ENDFOR
	 	\REPEAT
	 \STATE \(C_{cand} =\) all the clusters in \(U\) which overlap with \(C_a\)
		\STATE find a cluster \(S_i\) in \(C_{cand}\) with the smallest \(\frac{1}{\left|S_i\cup C_a\right| - \left|C_a\right|}\)
		\STATE \(C_a=C_a\bigcup S_i\)	
		\UNTIL {\(C_a\) covers \(V\)}
	\ENSURE \(C_a\)
\end{algorithmic}
\caption{Centralized algorithm 1}
\label{algo:greedy1}
\end{algorithm}

\begin{algorithm}
\begin{algorithmic}[1]
\REQUIRE \(G=(V, E)\) and parameters listed in Table. \ref{tab:Table2}
\STATE find \(n_{opt}\) which minimizes Eq. \ref{eq:nooverlap}
  \STATE The same with the 2 to 16 lines of Algorithm \ref{algo:greedy1}
  	\REPEAT
		\STATE find a cluster \(S_i\) in \(U\) with the smallest \(\frac{1}{\left|S_i\cup C_a\right| - \left|C_a\right|}\)
		\STATE \(C_a=C_a\bigcup S_i\)
	\UNTIL {\(C_a\) covers \(V\)}
	\STATE Identify Isolated cluster groups (ICGs) in \(C_a\)
	\STATE Construct a graph \(G_{ICG}=(V_{ICG}, E_{ICG})\):
 \STATE Run MST algorithm on \(G_{ICG}\) and get \(T\)
	\FORALL {edges in \(T\)}
		\STATE Create an extra cluster \(C_e\) and add it to \(C_a\)
	\ENDFOR
	\ENSURE \(C_a\)
\end{algorithmic}
\caption{Centralized algorithm 2}
\label{algo:greedy2}
\end{algorithm}

\begin{comment}
We will use a simple example to demonstrate the two algorithms above. Fig. \ref{fig:AlgorithmExampleNew}(a) plots a graph consisting of a total of 12 nodes. Assume we use the parameters associated with energy listed in Table 1 and the optimal \(n_{opt}\)=4. For simplicity, we also assume that the \(cond(S_i)<\gamma\) if \(\left|S_i\right| \geq p = 3\). Using the generated candidate cluster sets, 4 clusters are obtained using the first algorithm and illustrated in Fig. \ref{fig:AlgorithmExampleNew}(b). Note that all these 4 clusters are connected through the overlapping nodes. Using the second algorithm, a total of 3 clusters are generated after all the 12 nodes have been covered (see Fig. \ref{fig:AlgorithmExampleNew}(c)). However, these clusters do not overlap. Therefore, two extra clusters are generated to connect these isolated clusters. Note that since the minimum number of vibration patterns is 3, each extra cluster contains three nodes. The final clustering result of the second algorithm is shown in Fig. \ref{fig:AlgorithmExampleNew}(d). The total amount of energy using the generated clusters from the first algorithm is about \(0.88\) times of that of the second algorithm. 

\begin{figure}
	\centering
		\includegraphics[width=.3\textwidth,height=.35\textwidth]{AlgorithmExampleNew.eps}
	\caption{Example of using the Proposed Algorithms (a) Graph G(V,E), (b) The 4 Clusters Generated from the \(1^{st}\) Algorithm (c) Isolated Cluster Groups from the \(2^{nd}\) Algorithm (e) Final 5 Clusters from the \(2^{nd}\) Algorithm}
	\label{fig:AlgorithmExampleNew}
\end{figure}
\end{comment}

\subsection{A Distributed Algorithm}
Based on our first centralized algorithm, we propose a distributed solution. In this solution, each node only needs its one-hop neighbors information and communicates only with its one-hop neighbors. The clustering will start at a single controller node which usually is the sink node of the network.

Similar to Algorithm \ref{algo:greedy1}, each newly created cluster will be connected to at least one of the existing clusters. In the distributed algorithm, each node will maintain two lists of neighbors: unclustered and clustered. The lists will be sorted according to each neighbor's own number of unclustered neighbors. The nodes with fewer unclustered neighbors will do clustering or join other clusters first. This is done by assigning each node's execution of the algorithm to a specific time slot.

Each node has only three roles during clustering: unclustered, CM (cluster member) and CH. We illustrate the pseudo code based on the three roles in Algorithm \ref{algo:selectionsender}.

\begin{algorithm}
\begin{algorithmic}[1]
\REQUIRE \(n_{opt}\), \(p\), unclustered neighbors \(un\), clustered neighbors \(cn\) (\(un\) and \(cn\) are both sorted in increasing order according to the number of unclustered neighbors)
\IF {self is unclustered}
	\STATE Self becomes CH
\ENDIF
\IF {self is CH}
	\STATE Select one node from \(cn\) as its member
	\IF {\(size(un)\geq n_{opt}\)}
		\STATE Construct the cluster as size of \(n_{opt}\) by selecting first \(n_{opt}\) nodes from \(un\) as CM
	\ELSIF{\(n_{opt}>size(un)\geq p\)}
		\STATE Construct a cluster by selecting all nodes in \(un\) as CM
	\ELSE
		\STATE First construct a cluster by selecting all nodes in \(un\) as CM then select more nodes from \(cn\) as CM until \(cluster\_ size=p\)
	\ENDIF
\ELSIF {self is CM}
	\STATE Broadcast a message to all neighbors saying the status is currently CM.
\ENDIF
\STATE \(an=merge(un, cn)\)
\FORALL {\(n\in an\) that haven't been assigned a time slot}
	\STATE Assign time slot \(t[i]\) to \(n\) where \(i\) is the index of \(n\) in \(an\)
\ENDFOR
\end{algorithmic}
\caption{Distributed algorithm}
\label{algo:selectionsender}
\end{algorithm}

Each node will not execute the algorithm until the start of its own time slot. The input \(p\) is the minimum cluster size constraint and \(n_{opt}\) is the calculated optimal cluster size. Once a CH decides to choose certain node as CM, it will send a message \(req_{ch}\) and the corresponding CM will send \(acpt_{ch}\) to acknowledge the selection. After the execution of the algorithm on a node, an unclustered node will become CH. At the end of the algorithm, each node will merge all its neighbors into one sorted list and assign time slots. The duration of the time slot is large enough so that a CH can perform the selection. \(t[i]\) is the \(i^{th}\) time slot from the end of the current time slot. It can be easily seen that the generated clusters from the distributed algorithm can satisfy all the constraints.

\begin{comment}
Informally, it can be easily shown that the distributed algorithm is correct as the solution it produces will satisfy all the constraints:
\begin{itemize}
\item Since all nodes will be given a time slot to do run the algorithm, the clusters will eventually cover the whole network.
\item The clustering starts at the controller node and all the later CHs will first select a CM from its \(cn\). Therefore, all the clusters will be connected.
\item CH only selects CM within its one-hop neighbors, and the CHs will not remove its role as CH so all clusters will be one-hop.
\item For each CH, it will ensure the cluster size is at least \(p\) by selecting additional CM until the cluster size reaches either \(p\) or \(n_{opt}\).
\end{itemize}
\end{comment}